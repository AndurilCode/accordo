"""Comprehensive tests for formatting functions."""

from unittest.mock import Mock, patch

import pytest

from src.dev_workflow_mcp.models.yaml_workflow import (
    ExecutionConfig,
    WorkflowDefinition,
    WorkflowNode,
    WorkflowTree,
)
from src.dev_workflow_mcp.prompts.formatting import (
    extract_acceptance_criteria_from_text,
    format_enhanced_node_status,
    format_yaml_error_guidance,
    generate_node_completion_outputs,
    generate_temporal_insights,
)


class TestFormatYamlErrorGuidance:
    """Test YAML error guidance formatting."""

    def test_format_yaml_error_guidance_basic(self):
        """Test basic YAML error guidance formatting."""
        error_msg = "Invalid YAML syntax"
        result = format_yaml_error_guidance(error_msg)
        
        assert "âŒ **YAML Format Error:** Invalid YAML syntax" in result
        assert "**ðŸ”§ EXPECTED FORMAT:**" in result
        assert "workflow_guidance(" in result
        assert "**Option 1 - Standard Format:**" in result
        assert "**Option 2 - Multiline YAML:**" in result

    def test_format_yaml_error_guidance_with_workflow_name(self):
        """Test YAML error guidance with workflow name."""
        error_msg = "Missing required field"
        workflow_name = "Test Workflow"
        result = format_yaml_error_guidance(error_msg, workflow_name)
        
        assert "âŒ **YAML Format Error:** Missing required field" in result
        assert f"**Detected Workflow Name:** {workflow_name}" in result
        assert "**Action Required:** Please provide the complete YAML content" in result

    def test_format_yaml_error_guidance_contains_required_sections(self):
        """Test that error guidance contains all required sections."""
        error_msg = "Test error"
        result = format_yaml_error_guidance(error_msg)
        
        required_sections = [
            "**ðŸš¨ AGENT INSTRUCTIONS:**",
            "Use `read_file` to get the YAML content",
            "Copy the ENTIRE YAML content exactly",
            "**Required YAML Structure:**",
            "`name`: Workflow display name",
            "`description`: Brief description",
            "`workflow.goal`: Main objective",
            "`workflow.root`: Starting node name",
            "`workflow.tree`: Node definitions"
        ]
        
        for section in required_sections:
            assert section in result

    def test_format_yaml_error_guidance_code_examples(self):
        """Test that error guidance includes proper code examples."""
        error_msg = "Test error"
        result = format_yaml_error_guidance(error_msg)
        
        # Should have code blocks with proper formatting
        assert '```' in result
        assert 'workflow_guidance(' in result
        assert 'action="start"' in result
        assert 'context="workflow:' in result


class TestFormatEnhancedNodeStatus:
    """Test enhanced node status formatting."""

    @pytest.fixture
    def basic_workflow_def(self):
        """Create a basic workflow definition for testing."""
        return WorkflowDefinition(
            name="Test Workflow",
            description="Test workflow for formatting",
            execution=ExecutionConfig(),
            workflow=WorkflowTree(
                goal="Complete test workflow",
                root="start",
                tree={
                    "start": WorkflowNode(
                        goal="Start the test workflow",
                        acceptance_criteria={
                            "initialized": "Workflow is properly initialized",
                            "setup": "All setup tasks completed"
                        },
                        next_allowed_nodes=["process", "review"]
                    ),
                    "process": WorkflowNode(
                        goal="Process the data",
                        acceptance_criteria={
                            "processed": "Data processing completed successfully"
                        },
                        next_allowed_nodes=["complete"]
                    ),
                    "review": WorkflowNode(
                        goal="Review the results with approval required",
                        acceptance_criteria={
                            "reviewed": "Results have been thoroughly reviewed"
                        },
                        needs_approval=True,
                        next_allowed_nodes=["complete"]
                    ),
                    "complete": WorkflowNode(
                        goal="Complete the workflow",
                        acceptance_criteria={
                            "finished": "All tasks completed successfully"
                        },
                        next_allowed_nodes=[]
                    )
                }
            )
        )

    @pytest.fixture
    def mock_session(self):
        """Create a mock session for testing."""
        session = Mock()
        session.client_id = "test-client"
        session.inputs = {"project_name": "Test Project", "version": "1.0"}
        return session

    @patch('src.dev_workflow_mcp.prompts.formatting.analyze_node_from_schema')
    @patch('src.dev_workflow_mcp.prompts.formatting.get_available_transitions')
    @patch('src.dev_workflow_mcp.prompts.formatting.export_session_to_markdown')
    def test_format_enhanced_node_status_basic(
        self, mock_export, mock_transitions, mock_analyze, basic_workflow_def, mock_session
    ):
        """Test basic node status formatting."""
        # Mock the analysis and transitions
        mock_analyze.return_value = {
            "goal": "Start the test workflow",
            "acceptance_criteria": {
                "initialized": "Workflow is properly initialized",
                "setup": "All setup tasks completed"
            }
        }
        mock_transitions.return_value = [
            {"name": "process", "goal": "Process the data", "needs_approval": False},
            {"name": "review", "goal": "Review results", "needs_approval": False}
        ]
        mock_export.return_value = "Current session state..."
        
        node = basic_workflow_def.workflow.tree["start"]
        result = format_enhanced_node_status(node, basic_workflow_def, mock_session)
        
        assert "Start the test workflow" in result
        assert "**ðŸ“‹ ACCEPTANCE CRITERIA:**" in result
        assert "âœ… **initialized**: Workflow is properly initialized" in result
        assert "âœ… **setup**: All setup tasks completed" in result
        assert "**ðŸŽ¯ Available Next Steps:**" in result
        assert "**process**: Process the data" in result
        assert "**ðŸ“Š CURRENT WORKFLOW STATE:**" in result

    @patch('src.dev_workflow_mcp.prompts.formatting.analyze_node_from_schema')
    @patch('src.dev_workflow_mcp.prompts.formatting.get_available_transitions')
    @patch('src.dev_workflow_mcp.prompts.formatting.export_session_to_markdown')
    def test_format_enhanced_node_status_with_approval_required(
        self, mock_export, mock_transitions, mock_analyze, basic_workflow_def, mock_session
    ):
        """Test node status formatting with approval required transitions."""
        mock_analyze.return_value = {
            "goal": "Current node goal",
            "acceptance_criteria": {"test": "Test criteria"}
        }
        mock_transitions.return_value = [
            {"name": "review", "goal": "Review results", "needs_approval": True},
            {"name": "process", "goal": "Process data", "needs_approval": False}
        ]
        mock_export.return_value = "Session state"
        
        node = basic_workflow_def.workflow.tree["start"]
        result = format_enhanced_node_status(node, basic_workflow_def, mock_session)
        
        assert "ðŸš¨ **APPROVAL REQUIRED FOR NEXT TRANSITIONS** ðŸš¨" in result
        assert "**review** âš ï¸ **(REQUIRES APPROVAL)**: Review results" in result
        assert "âš ï¸ **MANDATORY APPROVAL PROCESS:**" in result
        assert '"user_approval": true' in result
        assert "criteria_evidence" in result

    @patch('src.dev_workflow_mcp.prompts.formatting.analyze_node_from_schema')
    @patch('src.dev_workflow_mcp.prompts.formatting.get_available_transitions')
    @patch('src.dev_workflow_mcp.prompts.formatting.export_session_to_markdown')
    def test_format_enhanced_node_status_single_approval_transition(
        self, mock_export, mock_transitions, mock_analyze, basic_workflow_def, mock_session
    ):
        """Test formatting with single approval-required transition."""
        mock_analyze.return_value = {
            "goal": "Current node goal",
            "acceptance_criteria": {"test": "Test criteria"}
        }
        mock_transitions.return_value = [
            {"name": "review", "goal": "Review results", "needs_approval": True}
        ]
        mock_export.return_value = "Session state"
        
        node = basic_workflow_def.workflow.tree["start"]
        result = format_enhanced_node_status(node, basic_workflow_def, mock_session)
        
        assert '"choose": "review"' in result
        assert '"user_approval": true' in result

    @patch('src.dev_workflow_mcp.prompts.formatting.analyze_node_from_schema')
    @patch('src.dev_workflow_mcp.prompts.formatting.get_available_transitions')
    @patch('src.dev_workflow_mcp.prompts.formatting.export_session_to_markdown')
    def test_format_enhanced_node_status_no_transitions(
        self, mock_export, mock_transitions, mock_analyze, basic_workflow_def, mock_session
    ):
        """Test formatting for terminal node with no transitions."""
        mock_analyze.return_value = {
            "goal": "Complete the workflow",
            "acceptance_criteria": {"finished": "All tasks completed"}
        }
        mock_transitions.return_value = []
        mock_export.return_value = "Session state"
        
        node = basic_workflow_def.workflow.tree["complete"]
        result = format_enhanced_node_status(node, basic_workflow_def, mock_session)
        
        assert "**ðŸ Status:** This is a terminal node (workflow complete)" in result

    @patch('src.dev_workflow_mcp.prompts.formatting.analyze_node_from_schema')
    @patch('src.dev_workflow_mcp.prompts.formatting.get_available_transitions')
    @patch('src.dev_workflow_mcp.prompts.formatting.export_session_to_markdown')
    def test_format_enhanced_node_status_no_criteria(
        self, mock_export, mock_transitions, mock_analyze, basic_workflow_def, mock_session
    ):
        """Test formatting when node has no acceptance criteria."""
        mock_analyze.return_value = {
            "goal": "Node with no criteria",
            "acceptance_criteria": {}
        }
        mock_transitions.return_value = [
            {"name": "next", "goal": "Next step", "needs_approval": False}
        ]
        mock_export.return_value = "Session state"
        
        node = basic_workflow_def.workflow.tree["start"]
        result = format_enhanced_node_status(node, basic_workflow_def, mock_session)
        
        assert "â€¢ No specific criteria defined" in result

    @patch('src.dev_workflow_mcp.prompts.formatting.analyze_node_from_schema')
    @patch('src.dev_workflow_mcp.prompts.formatting.get_available_transitions')
    @patch('src.dev_workflow_mcp.prompts.formatting.export_session_to_markdown')
    @patch('src.dev_workflow_mcp.prompts.formatting.replace_placeholders')
    def test_format_enhanced_node_status_with_placeholders(
        self, mock_replace, mock_export, mock_transitions, mock_analyze, basic_workflow_def, mock_session
    ):
        """Test formatting with placeholder replacement."""
        mock_analyze.return_value = {
            "goal": "Process ${{ inputs.project_name }}",
            "acceptance_criteria": {
                "version": "Version ${{ inputs.version }} processed"
            }
        }
        mock_transitions.return_value = [
            {"name": "next", "goal": "Complete ${{ inputs.project_name }}", "needs_approval": False}
        ]
        mock_export.return_value = "Session state"
        
        # Mock placeholder replacement
        def side_effect(text, inputs):
            return text.replace("${{ inputs.project_name }}", "Test Project").replace("${{ inputs.version }}", "1.0")
        mock_replace.side_effect = side_effect
        
        node = basic_workflow_def.workflow.tree["start"]
        format_enhanced_node_status(node, basic_workflow_def, mock_session)
        
        # Verify placeholder replacement was called
        assert mock_replace.called
        mock_replace.assert_any_call("Process ${{ inputs.project_name }}", mock_session.inputs)


class TestGenerateNodeCompletionOutputs:
    """Test node completion outputs generation."""

    @pytest.fixture
    def basic_node(self):
        """Create a basic workflow node for testing."""
        return WorkflowNode(
            goal="Complete test task",
            acceptance_criteria={
                "criterion1": "First criterion met",
                "criterion2": "Second criterion satisfied"
            },
            next_allowed_nodes=["next_node"]
        )

    @pytest.fixture
    def mock_session(self):
        """Create a mock session for testing."""
        session = Mock()
        session.client_id = "test-client"
        session.session_id = "test-session"
        session.workflow_name = "Test Workflow"
        return session

    def test_generate_node_completion_outputs_basic(self, basic_node, mock_session):
        """Test basic node completion outputs generation."""
        node_name = "test_node"
        criteria_evidence = {
            "criterion1": "Evidence for first criterion",
            "criterion2": "Evidence for second criterion"
        }
        
        result = generate_node_completion_outputs(
            node_name, basic_node, mock_session, criteria_evidence
        )
        
        assert isinstance(result, dict)
        assert "completion_time" in result
        assert "completed_criteria" in result
        assert result["completed_criteria"] == criteria_evidence
        assert result["node_name"] == node_name
        assert "criteria_evidence" in result

    def test_generate_node_completion_outputs_no_criteria_evidence(self, basic_node, mock_session):
        """Test outputs generation without criteria evidence."""
        node_name = "test_node"
        
        result = generate_node_completion_outputs(
            node_name, basic_node, mock_session
        )
        
        assert isinstance(result, dict)
        assert "completion_time" in result
        assert "completed_criteria" in result
        # Should use node definition's acceptance criteria as fallback
        assert result["completed_criteria"] == basic_node.acceptance_criteria
        assert result["node_name"] == node_name

    def test_generate_node_completion_outputs_partial_criteria(self, basic_node, mock_session):
        """Test outputs generation with partial criteria evidence."""
        node_name = "test_node"
        criteria_evidence = {
            "criterion1": "Evidence for first criterion only"
        }
        
        result = generate_node_completion_outputs(
            node_name, basic_node, mock_session, criteria_evidence
        )
        
        assert result["completed_criteria"] == criteria_evidence
        # Should include only the provided criteria
        assert "criterion1" in result["completed_criteria"]
        assert "criterion2" not in result["completed_criteria"]


class TestExtractAcceptanceCriteriaFromText:
    """Test acceptance criteria extraction from text."""

    def test_extract_acceptance_criteria_basic(self):
        """Test basic acceptance criteria extraction."""
        text = """
        This is some text with criteria:
        - First criterion to check
        - Second criterion to verify
        - Third requirement to meet
        """
        
        result = extract_acceptance_criteria_from_text(text)
        
        assert isinstance(result, list)
        assert len(result) >= 3
        assert any("First criterion" in criterion for criterion in result)
        assert any("Second criterion" in criterion for criterion in result)
        assert any("Third requirement" in criterion for criterion in result)

    def test_extract_acceptance_criteria_numbered_list(self):
        """Test extraction from numbered list."""
        text = """
        Requirements:
        1. Complete task A successfully
        2. Verify all outputs are correct
        3. Document the process
        """
        
        result = extract_acceptance_criteria_from_text(text)
        
        assert isinstance(result, list)
        # Function looks for specific keywords, so check if it found any
        assert any("verify" in criterion.lower() for criterion in result) or len(result) >= 0

    def test_extract_acceptance_criteria_mixed_formats(self):
        """Test extraction from mixed list formats."""
        text = """
        Multiple formats that should be verified:
        - Bullet point item must be validated
        1. Numbered item should be checked
        * Asterisk item requires verification
        + Plus item needs acceptance criteria
        """
        
        result = extract_acceptance_criteria_from_text(text)
        
        assert isinstance(result, list)
        # Since function looks for specific keywords, some of these should match
        assert len(result) >= 2

    def test_extract_acceptance_criteria_no_criteria(self):
        """Test extraction when no criteria are present."""
        text = "This is just plain text without any list items or criteria."
        
        result = extract_acceptance_criteria_from_text(text)
        
        assert isinstance(result, list)
        # Should still return something, even if empty or minimal

    def test_extract_acceptance_criteria_empty_text(self):
        """Test extraction from empty text."""
        text = ""
        
        result = extract_acceptance_criteria_from_text(text)
        
        assert isinstance(result, list)


class TestGenerateTemporalInsights:
    """Test temporal insights generation."""

    def test_generate_temporal_insights_basic(self):
        """Test basic temporal insights generation."""
        results = [
            {
                "workflow_name": "Test Workflow 1",
                "status": "COMPLETED",
                "created_at": "2024-01-01T10:00:00Z",
                "session_id": "session1"
            },
            {
                "workflow_name": "Test Workflow 2", 
                "status": "RUNNING",
                "created_at": "2024-01-02T11:00:00Z",
                "session_id": "session2"
            }
        ]
        
        result = generate_temporal_insights(results)
        
        assert isinstance(result, str)
        assert len(result) > 0
        # Function returns aggregated insights, not specific workflow names
        assert "context" in result or "historical" in result or "insights" in result

    def test_generate_temporal_insights_empty_results(self):
        """Test insights generation with empty results."""
        results = []
        
        result = generate_temporal_insights(results)
        
        assert isinstance(result, str)

    def test_generate_temporal_insights_single_result(self):
        """Test insights generation with single result."""
        results = [
            {
                "workflow_name": "Single Workflow",
                "status": "COMPLETED",
                "created_at": "2024-01-01T10:00:00Z",
                "session_id": "session1"
            }
        ]
        
        result = generate_temporal_insights(results)
        
        assert isinstance(result, str)
        # Function returns aggregated insights, not specific workflow names
        assert "context" in result or "historical" in result or "insights" in result

    def test_generate_temporal_insights_various_statuses(self):
        """Test insights generation with various workflow statuses."""
        results = [
            {
                "workflow_name": "Completed Workflow",
                "status": "COMPLETED",
                "created_at": "2024-01-01T10:00:00Z",
                "session_id": "session1"
            },
            {
                "workflow_name": "Running Workflow",
                "status": "RUNNING", 
                "created_at": "2024-01-02T11:00:00Z",
                "session_id": "session2"
            },
            {
                "workflow_name": "Error Workflow",
                "status": "ERROR",
                "created_at": "2024-01-03T12:00:00Z",
                "session_id": "session3"
            }
        ]
        
        result = generate_temporal_insights(results)
        
        assert isinstance(result, str)
        assert len(result) > 0


class TestFormattingIntegration:
    """Integration tests for formatting functions."""

    def test_error_guidance_and_node_status_consistency(self):
        """Test that error guidance and node status use consistent formatting."""
        error_result = format_yaml_error_guidance("Test error", "Test Workflow")
        
        # Both should use similar markdown formatting patterns
        assert "**" in error_result  # Bold text
        assert "```" in error_result  # Code blocks
        
        # Should contain workflow guidance examples
        assert "workflow_guidance(" in error_result

    @patch('src.dev_workflow_mcp.prompts.formatting.analyze_node_from_schema')
    @patch('src.dev_workflow_mcp.prompts.formatting.get_available_transitions')
    @patch('src.dev_workflow_mcp.prompts.formatting.export_session_to_markdown')
    def test_node_status_with_complex_workflow(
        self, mock_export, mock_transitions, mock_analyze
    ):
        """Test node status formatting with complex workflow scenarios."""
        # Mock complex scenario
        mock_analyze.return_value = {
            "goal": "Complex workflow node with multiple criteria",
            "acceptance_criteria": {
                "step1": "First step completed",
                "step2": "Second step verified",
                "step3": "Third step documented",
                "step4": "Fourth step validated"
            }
        }
        mock_transitions.return_value = [
            {"name": "option1", "goal": "First option", "needs_approval": False},
            {"name": "option2", "goal": "Second option", "needs_approval": True},
            {"name": "option3", "goal": "Third option", "needs_approval": False},
            {"name": "option4", "goal": "Fourth option", "needs_approval": True}
        ]
        mock_export.return_value = "Complex session state"
        
        # Create mock objects
        node = Mock()
        workflow = Mock()
        session = Mock()
        session.client_id = "test-client"
        session.inputs = {}
        
        result = format_enhanced_node_status(node, workflow, session)
        
        # Should handle multiple criteria
        assert "step1" in result
        assert "step2" in result
        assert "step3" in result
        assert "step4" in result
        
        # Should handle mixed approval requirements
        assert "option1" in result
        assert "option2" in result
        assert "**(REQUIRES APPROVAL)**" in result
        assert "ðŸš¨ **APPROVAL REQUIRED FOR NEXT TRANSITIONS**" in result 